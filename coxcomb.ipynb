{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-read in the 2.Indices_Target.csv file\n",
    "-filter on the NA-s in the snapshot_target column\n",
    "-get the unique target nums\n",
    "'''\n",
    "\n",
    "filename='2.Indices_Target.csv'\n",
    "df_target=pd.read_csv(f'C:/Users/511232/Desktop/Scorecard_coxcomb/{filename}', dtype='str')\n",
    "\n",
    "#filter out the dataframe where snapshot_target is NA\n",
    "df_na=df_target[df_target['snapshot_target'].isna()].copy()\n",
    "target_num_NA=set(df_na['Target_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code\tColor\n",
    "0\t    Grey\n",
    "1\t    Red\n",
    "2\t    Yellow\n",
    "3\t    Green\n",
    "\n",
    "-the scorecard.xlsx file, filter on color ('Color Scheme'=0) & ('Gap'=blank) & ('target' in target_num_NA)\n",
    "-groupby ['geoareaname', 'target'] and apply a function that will take the average of 'Trend'\n",
    "-and standardize the result of the average to be between [-10;+10]\n",
    "formula=N= -10 + [(A+100)/10]\n",
    "'''\n",
    "\n",
    "filename='scorecard.xlsx'\n",
    "#read in the scorecard.xlsx\n",
    "df=pd.read_excel('C:/Users/511232/Desktop/Scorecard_coxcomb/scorecard.xlsx', dtype=str)\n",
    "df['Trend'] = df['Trend'].astype(float)\n",
    "\n",
    "#flag as 1 if (df['Color Scheme']=='0') & (df['Gap'].isna()) & (df['target'].isin(target_num_NA)) condition meets\n",
    "cond=(df['Color Scheme']=='0') & (df['Gap'].isna()) & (df['target'].isin(target_num_NA))\n",
    "df.loc[cond,'flag']=1\n",
    "\n",
    "#filter color scheme=0 & Gap=blank\n",
    "df_filtered=df[(df['Color Scheme']=='0') & (df['Gap'].isna()) & (df['target'].isin(target_num_NA))].copy()\n",
    "\n",
    "\n",
    "def process(d):\n",
    "    '''will calculate average trend and standardize the value'''\n",
    "    trend_avg=d['Trend'].mean()\n",
    "    trend_avg_standardized=-10+((trend_avg+100)/10)\n",
    "    return(trend_avg_standardized)\n",
    "\n",
    "#apply the function on the df_filtered\n",
    "standardized_df=pd.DataFrame(df_filtered.groupby(['geoareaname','target'],group_keys=False).apply(process))\n",
    "standardized_df.reset_index(drop=False, inplace=True)\n",
    "standardized_df.columns=['geoareaname','target','standardized_value']\n",
    "#add flag=1\n",
    "standardized_df['flag']=1\n",
    "\n",
    "#merge the standardized_df with df\n",
    "result=pd.merge(df,standardized_df, how='left', on=['geoareaname','target', 'flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-generate coxcomb color using the following benchmarks:\n",
    "if N is negative --> red, takes the value of 1\n",
    "if N is postive but less or equal to 5.33 --> yellow, takes the value of 2\n",
    "if N is greater than 5.33--> green, takes the value of 3\"\n",
    "'''\n",
    "condition=[result['standardized_value']<0,(result['standardized_value']>0) & (result['standardized_value']<=5.33), result['standardized_value']>5.33]\n",
    "colors=[1,2,3]\n",
    "result['calculated_color']=np.select(condition,colors, default=np.nan)\n",
    "\n",
    "result.to_excel('processed_file.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discover the cutoff point for Yemen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/511232/Desktop/Scorecard_coxcomb')\n",
    "df=pd.read_excel('data Yemen.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_on_series(d):\n",
    "    # global idx\n",
    "    # global ix\n",
    "    \n",
    "    #get the categories labels\n",
    "    k=list(d.name)\n",
    "    k=[str(i) for i in k]\n",
    "\n",
    "    #report on the columns/names used in groupby\n",
    "    c=groupby_vars\n",
    "    categ_name=dict(zip(c,k))\n",
    "\n",
    "    #update a dictionary that will track number of datapoints for each groupby dataframes\n",
    "    series_count_key=f'{len(d)} datapoints'\n",
    "    if series_count_key in series_count.keys():\n",
    "        series_count[series_count_key]+=1\n",
    "    else:\n",
    "        series_count[series_count_key]=1\n",
    "\n",
    "    if len(d)>=10:\n",
    "                                    \n",
    "            '''\n",
    "            get the percentages of False and True from the boolean df['val'].pct_change()>0\n",
    "            b will be like the below\n",
    "            0     False\n",
    "            1     False\n",
    "            2     False\n",
    "            3     False\n",
    "            4      True\n",
    "            '''\n",
    "            #reset dataframe index\n",
    "            d.reset_index(inplace=True)\n",
    "            b=d['Value'].pct_change()>0\n",
    "            freq=b.value_counts(normalize=True)*100\n",
    "\n",
    "            '''check if True (or False) is between 45-65 %, that is if False and True are almost 50-50\n",
    "            then there is a possibility of one chunk from the timeseries having monotonic percent change sign'''\n",
    "            if True in freq.index:\n",
    "                if (freq[True]>=40) & (freq[True]<=60):\n",
    "                    '''get the index for ix of the True (or False)\n",
    "                    ix will be like Int64Index([4, 8, 11, 12, 13, 14, 15, 16, 18, 19, 20], dtype='int64')'''\n",
    "                    ix=b[b].index\n",
    "                    cutoff_year_min=float(d[\"TimePeriod\"][ix[0]])\n",
    "                    cutoff_year_max=float(d[\"TimePeriod\"][ix[-1]])\n",
    "                    cutoff_years.append([cutoff_year_min,cutoff_year_max])\n",
    "                 \n",
    "\n",
    "            result_per_serie.append({'series':s, 'categories':categ_name, 'number of data_points':len(d), 'cutoff':cutoff_years})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these from groupby vars\n",
    "list_to_exclude=['BasePeriod', 'TimeCoverage', 'Time_Detail','LowerBound', 'UpperBound', 'FootNote','GeoInfoUrl', 'Source','Reporting Type', 'Sampling Stations']\n",
    "\n",
    "#get unique SERIES\n",
    "unique_series=list(df['SeriesCode'].unique())\n",
    "\n",
    "result_per_serie=[] #initialize for each unique series\n",
    "series_count={}     #it will count indicators count\n",
    "cutoff_years=[]\n",
    "\n",
    "for s in unique_series:\n",
    "    #get the dataframe with SERIES == s\n",
    "    df_s=df[df['SeriesCode']==s].copy()\n",
    "    #get rid of null columns\n",
    "    df_s.dropna(how='all',axis=1, inplace=True)\n",
    "    #get the groupby vars which is the column names to groupby with\n",
    "    cols=df_s.columns\n",
    "    cols=set(cols).difference(set(list_to_exclude))\n",
    "    groupby_vars=list(cols.difference({'TimePeriod', 'Value'}))\n",
    "    # apply the function on the dataframe\n",
    "    df_s.groupby(groupby_vars, as_index=True).apply(report_on_series)\n",
    "\n",
    "\n",
    "#add the counter result to result_per_serie\n",
    "result_per_serie.append(series_count)\n",
    "\n",
    "#write it to a json file\n",
    "with open('report.json', 'w') as outfile:\n",
    "    json.dump(result_per_serie,outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "flat_year_list=pd.Series(list(chain.from_iterable(cutoff_years)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001.0    58\n",
       "2019.0    49\n",
       "2018.0    35\n",
       "2002.0    24\n",
       "2021.0    20\n",
       "2011.0    13\n",
       "2015.0    10\n",
       "2020.0    10\n",
       "2003.0     8\n",
       "2008.0     8\n",
       "2006.0     7\n",
       "2005.0     7\n",
       "2017.0     6\n",
       "2013.0     5\n",
       "2004.0     4\n",
       "2022.0     3\n",
       "2010.0     2\n",
       "2007.0     2\n",
       "2009.0     2\n",
       "2012.0     2\n",
       "2016.0     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_year_list.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coxcomb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
